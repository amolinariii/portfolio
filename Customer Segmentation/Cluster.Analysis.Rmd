---
title: "Customer Segmentation Analysis"
date: "8/10/2024"
author: "Arturo Molinar III"
output:
  pdf_document:
    df_print: kable
---
# 1. Abstract

&nbsp;&nbsp;&nbsp;The analysis of different subgroups within a data set can be explored
in many different ways. This report tests the abilities of Principal Component Analysis
to summarize data features and to use the Fitted Principal Components to help validate
the results from Clustering with K-means, K-modes, and K-prototypes by visualizing 
where the groups fall under. K-means focuses on numerical data, K-mode distinguishes groups
between categorical data, and K-prototype uses both categorical and numeric to find
distinct groups within a data set. This report aims to see which one performs
the best in finding clear and distinct subgroups within a data set using PCA to
help validate the results.


# 2. Introduction

&nbsp;&nbsp;&nbsp;Company X would like to learn more about the customers they serve and their
customers' purchasing behaviors based on attributes about them. Through this
analysis the company will better understand relationships among their
customers and be able to promote products and discounts to customers in an effort to boost sales.
&nbsp;&nbsp;&nbsp;Given data for 2,240 customers and 29 attributes, the goal is to summarize,
group, and find relationships between customers and their behaviors in order to
advertise deals or promote products. Through exploratory data analysis, principal 
component analysis, and clustering methods, relationships between customers and 
their spending behaviors will becomeprevalent. After grouping the customers and 
summarizing their characteristics, recommendations for marketing and sales will 
be presented.

\newpage
# 3. Importing, Understanding, and Feature Engineering Data Set

&nbsp;&nbsp;&nbsp;The data was originally from a csv file from Kaggle and uploaded into 
R as a data frame.

Link: https://www.kaggle.com/datasets/vishakhdapat/customer-segmentation-clustering

## i. Importing and Understanding the Data Set

&nbsp;&nbsp;&nbsp;Importing the data showed a change in format was needed to 
correct the data frame. Afterwards the column data types were corrected to their
respective numeric and factor types. A summary was ran on the data where 24 NA 
values were discovered for the `Income` feature. A recommender system was 
decided to be the best approach to fill in the missing values. Being that the 
clustering models thrive from interpreting variance and dissimilarities between
instances, it would be a better approach to the alternatives of filling in the 
missing values with the mean or leaving them out all together. There is enough 
data to build a recommender system to interpret the variances among the data 
and assign values.

## ii. Feature Engineering

&nbsp;&nbsp;&nbsp;The data types were changed to their respective categorical or numeric types.
The features `Year_joined`, `Kids`, and `Teens` were converted to factors 
as there is not much of a spread between the numbers to consider them numeric.
&nbsp;&nbsp;&nbsp;Features `Z_CostContact` and `Z_Revenue` are the same for each value so they 
will be removed from the rest of the features due to holding no additional
useful information.
&nbsp;&nbsp;&nbsp;The `Birth_year` feature has individuals who are over 100 years old, which 
does not make sense. These customers were removed from data as Age can play a 
huge role in determining behavior types. There is also an outlier for the 
`Income` feature which was also removed as it would skew the recommender system
and the clustering models. The rest of the features' values do not seem 
abnormal. 
&nbsp;&nbsp;&nbsp;Further reviewing the features has show more insights can be generated by 
feature engineering. `Income` does not affect the feature engineering that will 
be performed. `Birth_Year` will be converted into an Age to reduce the scale of 
the feature. Age will also help determine the generation each individual belongs
to which in return will help the clustering algorithms separate customers into 
groups and summarize their features. The Date can be split up into the year 
and month joined, then the month assigned into bins for their respective 
season joined. This information may help the algorithms differentiate between 
groups and add new layers of information to better summarize customers.

```{r, echo=FALSE, results='hide', message = FALSE, warning = FALSE}

# libraries used
library(tidyverse)
library(broom)
library(clustMixType)
library(forcats)
library(klaR)
library(janitor)

# File output preferances
knitr::opts_chunk$set(out.width = "50%", out.height="50%", fig.align="center", 
                      warning=FALSE, message=FALSE)
```
```{r, echo=FALSE,message=FALSE,results='hide'}
# set directory
setwd("C:/Users/molin/OneDrive/Desktop/Portfolio Projects/Customer Segmentation Analysis.Clustering/")

# data processing
customer_df <- read.csv("marketing_campaign.csv")  # kaggle's training set

# used to reset column labels
new_names <- "ID"
customer_df <- customer_df %>% set_names(new_names)

# full list of columns in dataframe
names <- c('ID', 'Birth_Year', 'Education', 'Marital_Status', 'Income', 
           'Kids', 'Teens', 'Dt_Customer', 'Recency', 'Wine_orders', 
           'Fruit_orders', 'Meat_orders', 'Fish_orders', 
           'Sweets_orders', 'Gold_orders', 'Deals_used', 
           'Web_Purchases', 'Catalog_Purchases', 'Store_Purchases', 
           'WebVisits_month', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 
           'AcceptedCmp1', 'AcceptedCmp2', 'Complain', 'Z_CostContact', 
           'Z_Revenue','Response')

# Change delimiter type
customer_df <- customer_df %>% 
  mutate(ID = str_replace_all(customer_df$ID, "\t", "."))

# Separate each entry into their respective columns
customer_df <- customer_df %>% 
  separate_wider_delim("ID", delim = '.', names = names)

# Separate and change data types of features based on numeric and category values.
numeric_frame <- customer_df %>% dplyr::select(-3, -4, -6:-8, -21:-26, -29)
numeric_frame <- as.data.frame(sapply(numeric_frame, as.numeric))

# extract categorical and convert to factor
category_frame <- customer_df %>% dplyr::select(3:4, 6:8, 21:26, 29)
category_frame <- as.data.frame(sapply(category_frame, as.factor))

# format date column and change data type
category_frame$Dt_Customer <- as.Date(category_frame$Dt_Customer, 
                                      format = "%d-%m-%Y")

# Combine processed features
customer_df <- cbind(numeric_frame, category_frame)
print(customer_df[1:5,1:5])
 

# summary of data, and extraction of NA values and anomalies
summary(customer_df)

# Subset Birth_Year anomalies
age_anamolies <- customer_df[customer_df$Birth_Year < 1934, ]
age_anamolies[, 1:3]
income_outliers <- customer_df[!is.na(customer_df$Income) & customer_df$Income > 300000, ]
income_outliers

# retain original df and remove anomalies/outliers from the original dataframe, 
original_df <-customer_df
customer_df <- customer_df[!(customer_df$Birth_Year < 1934), ]
customer_df <- customer_df[!(!is.na(customer_df$Income) & customer_df$Income > 300000), ]

# Add column for row index
customer_df$row_id <- 1:nrow(customer_df)

# Pull rows with NA in them
na_rows <- customer_df %>%
  dplyr::filter(if_any(everything(), is.na))

# Store row_id placements of where na values are found in list
na_list <- c(na_rows$row_id)

# Get current year to create age
current_date <- Sys.Date()
current_year <- as.integer(format(current_date, "%Y"))

# Feature engineer
customer_df <- customer_df %>% mutate(
  Age = current_year - Birth_Year,
  Generation = if_else((Birth_Year <= 1945), "Silent", 
                         if_else(Birth_Year <= 1964, "Boomer",
                         if_else(Birth_Year <= 1980, "Gen X", 
                         if_else(Birth_Year <= 1996, "Millennial",
                         if_else(Birth_Year <= 2006, "Gen Z",
                         "Minor"))))),
  Year_joined = as.integer(format(Dt_Customer, "%Y")),
  Month_joined = as.integer(format(Dt_Customer, "%m")),
  Season_joined = if_else(Month_joined <= 2, "Winter",
                          if_else(Month_joined <= 5, "Spring",
                          if_else(Month_joined <= 8, "Summer",
                          if_else(Month_joined <= 11, "Fall",
                                  "Winter"))))
  )
# remove unnecessary features after manipulation
customer_df <- customer_df %>% dplyr::select(-1:-2, -16:-17, -22, -30)
head(customer_df)

```
\newpage

# 4. Recommender System to Fill in Missing Values

&nbsp;&nbsp;&nbsp;The recommender system only applies to numerical data so the features have been
separated accordingly. The missing values have been identified and have been replaced
with the mean for the time being. Principal Component Analysis was then performed
to get a Scree Plot of the amount of variance explained by each eigenvector.

```{r, echo=FALSE, out.width="100%"}

# Subset factor features from customer_df to cbind with numeric afterwards
factor_frame <- customer_df %>% dplyr::select(14:24, 26:29)
factor_frame <- as.data.frame(lapply(factor_frame, as.factor))

# subset numeric data with NA values to recommend
recommender_df <- customer_df %>% dplyr::select(-14:-24, -26:-29)

# Custom function to scale a single column, handling NA values
scale_na <- function(x) {
  x_centered <- x - mean(x, na.rm = TRUE)
  x_scaled <- x_centered / sd(x, na.rm = TRUE)
  return(x_scaled)
}

# Apply the custom scaling function to each column of the dataframe
X_tilde <- as.data.frame(lapply(recommender_df, scale_na))

# Find where NA values exist
not_na <- is.na(X_tilde)
na <- !not_na

# Get means of each numeric column
col_means <- colMeans(X_tilde, na.rm = TRUE)

# assign means to all NA values
X_tilde[not_na] <- col_means[col(X_tilde)[not_na]]

# Verified no NA values are present anymore
na_rows <- X_tilde %>%
  dplyr::filter(if_any(everything(), is.na))

# Scale is off due to variables being scaled already
prc_recommender <- X_tilde %>% prcomp(scale=FALSE)
pc_sd <- prc_recommender$sdev
var_explained <- (pc_sd)^2 / sum(pc_sd^2)

# Create a data frame for the variance explained
df_var_explained <- data.frame(
  Principal_Component = seq_along(var_explained),  # Sequence along the vector
  Variance_Explained = var_explained               # Variance explained values
)

# Plot of Principal Components
ggplot(df_var_explained, aes(x = Principal_Component, y = Variance_Explained)) +
  geom_point(size = 3) +                             # Add points
  geom_line(group = 1) +                             # Add a line connecting the points
  labs(x = "Principal Component",                    # X-axis label
       y = "Proportion of Variance Explained",       # Y-axis label
       title = "Scree Plot")                       # Plot title
```


&nbsp;&nbsp;&nbsp;Through the Scree plot and the amount of variance explained by each PC, three
is the preferred amount of eigenvectors to use in the recommender system as it
will explain approximately 60% of the variance and creates an 'elbow' at three
eigenvectors. The significance of the elbow tells us that any further eigenvectors
will not add much value to the summarizing of the data.

&nbsp;&nbsp;&nbsp;The recommender system was built by extracting the three eigenvectors
and creating a centered matrix of the scaled data. Then an approximation matrix was
generated by matrix multiplication between the scaled matrix with the extracted
eigenvector matrix and the transpose of the eignevalue matrix. The mean was then
added back and the generated values were used to updated the original matrix. 
This process was repeated until the objective function converged.

```{r, echo=FALSE}
# Number of eigenvectors that will be used
M <- 3
counter <- 0
# Iteration of matrix completion
for (i in 1:4) {
# Calculate PCA
pca_result <- prcomp(X_tilde, center = TRUE, scale. = FALSE)

# Extract the first M eigenvectors
V_app <- pca_result$rotation[, 1:M]

# creates centered data matrix
Xc <- scale(X_tilde, center = TRUE, scale = FALSE)

# X approximate
X_app <- Xc %*% V_app %*% t(V_app)

# Add back the mean
centering <- pca_result$center
X_app <- scale(X_app, center =-centering, scale = FALSE)

# Update matrix to X_old
X_old <- X_tilde
X_tilde[not_na] <- X_app[not_na]

# Compute objective
Err <- sum((X_tilde[na]- X_app[na])^2)
counter <- counter+1
}
cat("The objective function converged at", counter, "iterations and has error:", Err, "\n")
```

&nbsp;&nbsp;&nbsp;The recommender matrix was unscaled and the following plot shows
the values generated compared to the mean.


```{r, echo=FALSE,out.width="100%"}
# Custom unscale function
unscale <- function(x, original) {
  x_unscaled <- x * sd(original, na.rm = TRUE)
  x_uncentered <- x_unscaled + mean(original, na.rm = TRUE)
  
  return(x_uncentered)
}

# Apply unscale f(x) to each column of X_tilde with values from the original df
final_recommender <- as.data.frame(mapply(unscale, X_tilde, recommender_df, 
                                          SIMPLIFY = FALSE))
# final_recommender[1:5,1:5]

# Use column mean of original df to compare with recommender values
col_means <- colMeans(recommender_df, na.rm = TRUE)
cat("The mean for the Income column before recommender system was:", 
    col_means[1], "\n")

# Add column for row index and subset na_list from earlier based on index
final_recommender$row_id <- 1:nrow(final_recommender)
subset_df <- final_recommender[final_recommender$row_id %in% na_list, ]

# Ensure subset_df$Income is sorted
sorted_income <- sort(subset_df$Income)

# Create a sequence from 1 to the length of sorted_income
index_sequence <- seq_along(sorted_income)

# show results
print('These are the plotted returned values from the recommender system.')
print('The blue line represents the mean before applying the recommender system')

# Create data frame for sorted income values
df_income <- data.frame(
  Index = index_sequence,
  Income = sorted_income
)
# plot of income values
ggplot(df_income, aes(x = Index, y = Income)) +
  geom_point() +                                        # Add points for each data point
  geom_line() +                                         # Add a line connecting the points
  geom_hline(yintercept = col_means[1], color = "blue") +  # Add a horizontal line at the mean
  labs(x = "Index", y = "Income", title = "Recommender Income Plotted Values") +  # Labels and title
  xlim(0, length(sorted_income) + 1) +                  # Set x-axis range
  ylim(0, max(sorted_income))

# Join recommended and factor dataframe to get complete dataframe
cleaned_customer_df <- cbind(final_recommender, factor_frame)

# Round the Income to nearest whole number
cleaned_customer_df <- cleaned_customer_df %>% mutate(
  Income = floor(Income)
)

# take out row ID
cleaned_customer_df <- cleaned_customer_df %>% dplyr::select(-15)

# Input row index again
rownames(cleaned_customer_df) <- seq_len(nrow(cleaned_customer_df))
```


&nbsp;&nbsp;&nbsp;Given the initial mean and the recommended values, the recommender system
successfully recommended values for the missing values within the data based on
the features and distributions of each customer within the data set. As shown on the graph the
values are distributed in a linear matter suggesting it has successfully interpreted
the relationships among the variables making it more reliable than skewing the 
data by imputing the mean into each missing value.
\newpage

# 5. Exploratory Data Analysis

&nbsp;&nbsp;&nbsp;Now that all of the missing values have been dealt with, the features have been engineered, and unnecessary features have been removed, exploratory data analysis can be performed to evaluate distributions, check correlations, and create frequency tables.

```{r, echo=FALSE, results='hide'}

# Separate dataframes and scale numerical data
num <- cleaned_customer_df %>% dplyr::select(where(is.numeric)) %>% scale()
fact <- cleaned_customer_df %>% dplyr::select(where(is.factor))

# Set up the plotting area
par(mfrow=c(2, 3))
par(bg="white")

# Loop through each column in the standardized numeric data
for (i in 1:ncol(num)) {
  # Calculate density
  d <- density(num[, i]) # Use num[, i] to access the i-th column
  
  # Plot the density, remove "#" when needed
 # plot(d, main = colnames(num)[i]) # Use colnames(num)[i] to get the column name
}

# evaluate correlation matrix
cor_matrix <- cor(num)
# cor_matrix

# Loop through each column in the dataframe
for (col in names(fact)) {
    # Get frequencies for plots
    counts <- sort(table(fact[[col]]), decreasing = TRUE)
    cat("\nFeature:", col, "\n")
    print(counts)
}

```

Summary of Distributions:

&nbsp;&nbsp;&nbsp;The distributions for the numerical data are skewed and multimodal. In this instance the 
distributions will not drastically change the performance of the clustering 
algorithms and the use of PCA will help summarize the data. It is noted that most 
variables are weakly or moderately correlated.

Summary of Frequency tables:

&nbsp;&nbsp;&nbsp;The majority of customers are educated. Many are in a relationship or single,
and some are divorced or widowed. Many of the customers are Gen X or Baby
Boomers. Most customers joined in 2013. The most popular month that customers
joined was in August while the least popular month to join was in July. The
most popular time of year for customers to sign up was in Spring while Summer
had the least. This could be due to advertising or the products offered by the
company not meeting customer needs.
&nbsp;&nbsp;&nbsp;Anomalies for the Martial Status were found. Absurd, YOLO, and Alone values
will be replaced with 'Single' status.

Plotting features to identify if any subgroups can be determined:

&nbsp;&nbsp;&nbsp;To see if there may be any relationships that can be identified before
clustering or summarizing the data, a plot was made for the `Income`, `Recency`,
and `Year Joined`. 

```{r, echo=FALSE,out.width="100%"}
### Clean categorical data types, remove outliers
cleaned_customer_df <- cleaned_customer_df %>% mutate(
  Marital_Status = if_else(Marital_Status %in% c('YOLO', 'Absurd', 'Alone'),
                           'Single', Marital_Status)
)
cleaned_customer_df$Marital_Status <- as.factor(cleaned_customer_df$Marital_Status)

fact <- fact %>% mutate(
  Marital_Status = if_else(Marital_Status %in% c('YOLO', 'Absurd', 'Alone'),
                           'Single', Marital_Status)
)
fact$Marital_Status <- as.factor(fact$Marital_Status)

df_scaled <- cleaned_customer_df %>% dplyr::select(where(is.numeric)) %>% scale()
fact <- cleaned_customer_df %>% dplyr::select(where(is.factor))

# create scatter plot to evaluate if any relationship can be determined without other methods
ggplot(cleaned_customer_df, aes(y = Recency, x = Income)) +
  geom_jitter(aes(colour = factor(Year_joined))) +
  labs(
    title = "Comparing Income and Recency by Year Joined",  # Add the title
    x = "Income",  # Label for the x-axis
    y = "Recency", # Label for the y-axis
    colour = "Year Joined"  # Legend title for the color grouping
  )

```

&nbsp;&nbsp;&nbsp; Unfortunately there is no clear relationship between important 
features so clustering and summarizing the customers would help show distinctions between them.
\newpage
# 4. Principal Component Analysis

&nbsp;&nbsp;&nbsp;PCA will be performed again now that the data is complete. The
Principal Components will then be analyzed to see what features each of them is 
summarizing.

&nbsp;&nbsp;&nbsp;The resulting Scree plot shows three principal components are enough to explain the
majority of the variance. Roughly 60% of the variance is explained within the
first three Principal Component values.

```{r, echo=FALSE,out.width="100%"}
# Create scree plot based on variance explained by each eigen vector
pca <- df_scaled %>% prcomp(scale=FALSE)
pca_sd <- pca$sdev
variance_explained <- (pca_sd)^2 / sum(pca_sd^2)

df_var_explained <- data.frame(
  Principal_Component = seq_along(var_explained),  # Sequence along the vector
  Variance_Explained = var_explained               # Variance explained values
)

ggplot(df_var_explained, aes(x = Principal_Component, y = Variance_Explained)) +
  geom_point(size = 3) +                             # Add points
  geom_line(group = 1) +                             # Add a line connecting the points
  labs(x = "Principal Component",                    # X-axis label
       y = "Proportion of Variance Explained",       # Y-axis label
       title = "Scree Plot")

PVE_table <- data.frame(variance_explained,
cumsum(variance_explained)
)
```


&nbsp;&nbsp;&nbsp;The first five eigenvectors are added to the original
data frame and their values are shown below.


```{r, echo = FALSE,out.width="100%"}
## Analysis of principal components

# create dataframe to store PC's
master_df <- cleaned_customer_df
# Show first three PC's
(abs(pca$rotation[,1:5]) > 0.3)*pca$rotation[,1:5] %>% round(4)

# add PC's to dataframe
org_augmented <- augment(pca, master_df)

```

Summary of PCA:

&nbsp;&nbsp;&nbsp;The first PC summarizes customers who do not have a large income and hardly
order wine and meat products. They also do not buy that often from the catalog
or in store, and do not usually go for discounted products. These customers can 
best be described as having lower incomes and do not make that many purchases, 
especially on wines and meats.

&nbsp;&nbsp;&nbsp;The second PC summarizes customers who do not purchase products with deals
or make a lot of web orders. They also do not visit the website that often
either. This suggests they are customers who buy more in store than online.

&nbsp;&nbsp;&nbsp;The third PC summarizes the age of customers and their income. 
Customers with a large third PC are generally older than other customers and
tend to have slightly larger incomes.

&nbsp;&nbsp;&nbsp;The fourth PC summarizes the last time customers have ordered. Customers with
a larger fourth PC have not ordered in a while.

&nbsp;&nbsp;&nbsp;The fifth PC summarizes customers age and spending on meat products. These 
customers are younger and rarely buy meat products.

&nbsp;&nbsp;&nbsp;The Principal Components alone do not offer enough insight into
possible segmentation of the customers. Plotting them may further help identify 
groups.


```{r, echo = FALSE,out.width="100%"}
# Graph of first two PC's
ggplot(org_augmented, aes(y = .fittedPC1, x = .fittedPC2)) + geom_point() +
  labs(title = "K-mean Clusters plotted on Principal Components",
       x = "Principal Component 1",
       y = "Principal Component 2") 
```

PC Plot Analysis:

&nbsp;&nbsp;&nbsp;Given the plotted principal components, it looks like there could be three
distinct subgroups among the customers. It is not clear what rule could help
discriminate the groups so clustering will be needed to further help subset the 
customers and provide insight as to why their behaviors are similar.
\newpage

# 5. Clustering: K-means, K-mode, K-prototype

## i. K-means Clustering

&nbsp;&nbsp;&nbsp;Clustering by K-means will be performed first on the numerical
data to see if subgroups can be determined with those features alone. A Scree Plot
of the objective function will help determine how many clusters we should expect.

```{r, echo=FALSE,out.width="100%"}
# k-means scree plot
within_ss <- rep(NA, ncol(df_scaled))
set.seed(4990) # it uses random initialization
for(i in 1:ncol(df_scaled)){
k_mean <- kmeans(x=df_scaled,
                    i,
                    nstart = 50,
                    iter.max = 15)
within_ss[i] <- k_mean$tot.withinss
}

# create data frame for the objective values
df_scree <- data.frame(
  Clusters = 1:ncol(df_scaled),  # Number of clusters
  Objective_Function = within_ss # Within-cluster sum of squares
)
# plot objective function values
ggplot(df_scree, aes(x = Clusters, y = Objective_Function)) +
  geom_point() +  # Add points for each cluster number
  geom_line() +   # Add lines connecting the points
  labs(
    x = "# Clusters",  # X-axis label
    y = "Objective Function",  # Y-axis label
    title = "Scree Plot"  # Plot title
  )
```

&nbsp;&nbsp;&nbsp;It is shown in the Scree Plot that an elbow occurs at three
clusters. A model will be fitted with three fixed clusters and graphed onto the
first two Principal Components

```{r, echo = FALSE,out.width="100%"}

# Perform k-means on features with 3 clusters
set.seed(4990)
kmeans_cluster_3 <- invisible(kmeans(x=df_scaled,
centers = 3,
nstart = 10))

# assign clusters to augmented df
org_augmented$kmeans_cluster_3 <- as.factor(kmeans_cluster_3$cluster)
cleaned_customer_df$kmeans_cluster_3 <- as.factor(kmeans_cluster_3$cluster)

# plot of clusters found from k-means on PC's
ggplot(org_augmented, aes(y = .fittedPC1, x = .fittedPC2)) + 
  geom_jitter(aes(colour = factor(kmeans_cluster_3))) +
  labs(title = "K-means Clusters plotted on Principal Components",
       x = "Principal Component 1",  
       y = "Principal Component 2")  
```

&nbsp;&nbsp;&nbsp;The plotted Principal Components distinguished by the groups
the K-means model created shows three distinct groups. Furthermore, plotting
the `Income` and `Recency` features from earlier distinguished by the clusters
shows a better distinction of the groups.

```{r, echo = FALSE,out.width="100%"}
# create scatter plot to show cluster relationship between features
ggplot(org_augmented, aes(y = Recency, x = Income)) +
  geom_jitter(aes(colour = factor(kmeans_cluster_3))) +
  labs(
    title = "Comparing Income and Recency by Clusters",  # Add the title
    x = "Income",  # Label for the x-axis
    y = "Recency", # Label for the y-axis
    colour = "K-means Clusters"  # Legend title for the color grouping
  )
```

&nbsp;&nbsp;&nbsp;Compared to the original plot of `Income` and `Recency`, the clusters
found by K-means does an excellent job separating the customers into subgroups.
The graph now shows clear distinct groups with some variance between them. K-modes
will be evaluated next to see if it yields similar results.

\newpage
## ii. K-modes Clustering

&nbsp;&nbsp;Clustering by K-modes will be performed next on the categorical
data to see if subgroups can be determined with those features alone. A Scree Plot
of the objective function will help determine how many clusters we should expect.

```{r, echo = FALSE,out.width="100%"}
### K-modes Clustering

# Initialize a vector to store the objective function values
within_ss_mode <- rep(NA, ncol(fact))

set.seed(4990)  # Set seed for reproducibility
# Loop over the number of clusters
invisible(for (i in 1:ncol(fact)) {
  k_modes <- kmodes(fact, modes = i, weighted = FALSE)

  # Sum of within-cluster dissimilarities (objective function)
  within_ss_mode[i] <- sum(k_modes$withindiff)
})

# create data frame for objective function
df_scree <- data.frame(
  Clusters = 1:ncol(fact),          # Number of clusters
  Objective_Function = within_ss_mode # Within-cluster sum of squares
)

# plot scree plot of objective function
ggplot(df_scree, aes(x = Clusters, y = Objective_Function)) +
  geom_point() +  # Add points for each cluster number
  geom_line() +   # Add lines connecting the points
  labs(
    x = "# Clusters",                # X-axis label
    y = "Objective Function",        # Y-axis label
    title = "Scree Plot of K-modes Objective Function"   # Plot title
  ) 
```


&nbsp;&nbsp;The objective function scree plot shows three clusters would be optimal
for the model to asses. Setting the clusters equal to three shows the following results.


```{r, echo = FALSE,out.width="100%"}
org_fact <- fact
set.seed(4990)
fact_cluster_3 <- invisible(kmodes(fact, modes = 3, 
                         weighted = FALSE))
# assign clusters to augmented df
fact$kmode_lust_3 <- as.factor(fact_cluster_3$cluster)
cleaned_customer_df$kmode_clust_3 <- as.factor(fact_cluster_3$cluster)
org_augmented$kmode_clust_3 <- as.factor(fact_cluster_3$cluster)

ggplot(org_augmented, aes(y = .fittedPC1, x = .fittedPC2)) + 
  geom_jitter(aes(colour = factor(kmode_clust_3))) +
  labs(title = "K-modes Clusters plotted on Principal Components",
       x = "Principal Component 1",  
       y = "Principal Component 2") 
```

&nbsp;&nbsp;Using the Principal Components to understand the ditribution of the clusters
does not show a clear distinction between the clusters. There is a lot of variance between
the groups which suggests PCA cannot help validate the results found from K-modes.

```{r, echo=FALSE,out.width="100%"}
ggplot(org_augmented, aes(x = Income, y = Recency, 
                                color = kmode_clust_3)) + 
  geom_jitter() +
  labs(
    title = "Comparing Income and Recency by Clusters",  # Add the title
    x = "Income",  # Label for the x-axis
    y = "Recency", # Label for the y-axis
    colour = "K-modes Clusters"  # Legend title for the color grouping
  )

```

&nbsp;&nbsp;Furthermore, the graph of `Income` vs `Recency` separated by clusters
does not yield clear results of where subgroups may exist. Further analysis into
the Categorical groupings of the data set is needed to evaluate why these clusters
were subgrouped the way they were. Before getting into the analysis of the K-modes clusters,
it is best to see what results are yeilded by K-prototypes

## iii. K-prototypes Clustering

&nbsp;&nbsp;Clustering by K-prototypes will be performed next on both the categorical
and numerical data within the data set to determine which subgroups exist among all the
features being accounted for. A Scree Plot of the objective function will help 
determine how many clusters we should expect to have.

```{r, echo=FALSE,results='hide',message = FALSE, warning = FALSE, out.width="100%"}
scaled_customer_df <- cbind(df_scaled, org_fact)

# Initialize a vector to store the objective function values
within_ss_proto <- rep(NA, ncol(scaled_customer_df))

set.seed(4990)  # Set seed for reproducibility

# Loop over the number of clusters
invisible(for (i in 1:ncol(scaled_customer_df)) {
  # Apply k-prototypes clustering
  k_prototypes <- kproto(scaled_customer_df, k = i)
  
  # Sum of within-cluster dissimilarities (objective function)
  within_ss_proto[i] <- k_prototypes$tot.withinss
})
```
```{r, echo=FALSE, message = FALSE, warning = FALSE, out.width="100%"}
# create data frame for objective function
df_scree <- data.frame(
  Clusters = 1:ncol(scaled_customer_df),          # Number of clusters
  Objective_Function = within_ss_proto # Within-cluster sum of squares
)

# plot objective function values
ggplot(df_scree, aes(x = Clusters, y = Objective_Function)) +
  geom_point() +  # Add points for each cluster number
  geom_line() +   # Add lines connecting the points
  labs(
    x = "# Clusters",                # X-axis label
    y = "Objective Function",        # Y-axis label
    title = "Scree Plot of K-prototypes Objective Function"   # Plot title
  ) 
```

&nbsp;&nbsp;The objective function scree plot shows three clusters would be optimal
for the model to asses. Setting the clusters equal to three shows the following results.

```{r, echo=FALSE, results = "hide",message = FALSE, warning = FALSE, out.width="100%"}

# Set seed for reproducibility
set.seed(4990)
proto_cluster_3 <- invisible(kproto(scaled_customer_df, k = 3))

# Assign clusters to augmented df
org_augmented$kproto_clust_3 <- as.factor(proto_cluster_3$cluster)
cleaned_customer_df$kproto_clust_3 <- as.factor(proto_cluster_3$cluster)
```
```{r, echo=FALSE, message = FALSE, warning = FALSE, fig.width=6, fig.height=6}

# Plot k-prototypes on Principal components
ggplot(org_augmented, aes(y = .fittedPC1, x = .fittedPC2)) + 
  geom_jitter(aes(colour = factor(kproto_clust_3))) +
  labs(title = "K-prototypes Clusters plotted on Principal Components",
       x = "Principal Component 1",  
       y = "Principal Component 2") 

# split clusters up into different data frames for analysis
k_means_3 <- org_augmented %>% dplyr::select(-1,-31:-44, -46:-47)
k_modes_3 <- org_augmented %>% dplyr::select(-1,-31:-44, -45,-47)
k_prototypes_3 <- org_augmented %>% dplyr::select(-1,-31:-46)

```

&nbsp;&nbsp;The results show clear subgroups within the data set and the groups are
very similar to the results of K-means from earlier. There is some overlapping variance
among the groups but given it takes into consideration the categorical features this
method is superior then the others.

```{r,echo = FALSE,out.width="100%"}
ggplot(org_augmented, aes(x = Income, y = Recency, 
                                color = kproto_clust_3)) + 
  geom_jitter() +
  labs(
    title = "Comparing Income and Recency by K-prototypes Clusters",  # Add the title
    x = "Income",  # Label for the x-axis
    y = "Recency", # Label for the y-axis
    colour = "K-prototypes Clusters"  # Legend title for the color grouping
  )

```

&nbsp;&nbsp;&nbsp;Compared to the original plot of `Income` and `Recency`, the clusters
found by K-prototypes does an excellent job separating the customers into subgroups.
The graph now shows clear distinct groups with some variance between them. This further
validates K-prototypes as the more optimal model in separating the customers into
subgroups. In the next section an analysis of all three models' results will be interpreted
to get a clear picture as to why these clusters are different from each model.

# 6. Analyzing Clusters

* Functions used to plot and analyze data from clusters

```{r, echo=FALSE, out.width="100%"}
grouped_bar_chart <- function(df, cluster_column) {
  for (col in names(df)) {
    if (col == cluster_column) {
      next  # Skip the cluster column
    } else {
      # Check if the current column is a factor
      if (is.factor(df[[col]])) {
        # Create a grouped bar chart with counts
        print(
          ggplot(df, aes_string(x = col, fill = cluster_column)) +
            geom_bar(stat = "count", position = "dodge") +
            labs(title = paste("Grouped Bar Chart for", col), x = col, y = "Count") +
            theme_minimal()
        )
      } else {
        next  # Skip non-factor columns
      }
    }
  }
}

grouped_stacked_plot <- function(df, cluster_column_name) {
  # Extract the cluster column
  cluster_column <- df[[cluster_column_name]]
  
  for (col in names(df)) {
    if (col == cluster_column_name) {
      next  # Skip the cluster column
    } else {
      # Check if the current column is a factor
      if (is.factor(df[[col]])) {
        
        contingency_table <- table(df[[col]], cluster_column)
        df_counts <- as.data.frame(contingency_table)
        names(df_counts) <- c(col, cluster_column_name, "Count")
        
        # Create a stacked bar chart and plot
        print(ggplot(df_counts, aes_string(x = col, y = "Count", fill = cluster_column_name)) +
          geom_bar(stat = "identity", position = "stack") +  # Use stat = "identity" to use provided counts
          labs(title = paste("Stacked Bar Chart of", col, "with Clusters"), 
               x = col, 
               y = "Count") +
          theme_minimal()
        )
      } else {
        next  # Skip non-factor columns
      }
    }
  }
}

box_plots <- function(df, cluster) {
  
  # Add the cluster column dynamically based on the argument
  numeric_data <- df %>%
    dplyr::select_if(is.numeric) %>%
    dplyr::mutate(cluster_col = df[[cluster]]) %>%  # Dynamically add the cluster column
    gather(var, value, -cluster_col)  # Gather the data into long format
  
  # Create the boxplot using ggplot
  ggplot(numeric_data, aes(group = cluster_col, y = value, fill = cluster_col)) +
    geom_boxplot() +
    labs(title = "Numerical Boxplots by Cluster ") +
    facet_wrap(~var, scales = "free_y", ncol = 4) +
    scale_y_continuous(n.breaks = 3)
}

```
\newpage
# 7. Findings